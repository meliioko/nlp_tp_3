{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8e1992ac-b345-4bbd-a621-538b6a982f58",
      "metadata": {
        "id": "8e1992ac-b345-4bbd-a621-538b6a982f58"
      },
      "source": [
        "# Using GloVe for text classification\n",
        "\n",
        "In this pre-filled notebook, we use GloVe embeddings to train a classifier for sentiment analysis. For every review in the IMDB dataset we:\n",
        "1. Tokenize the review into tokens.\n",
        "2. Get the pre-trained GloVe vector for every token in the review (if they are in the voabulary of GloVe).\n",
        "3. Average the vectors over the full review.\n",
        "4. Send the vector through a logistic regression.\n",
        "\n",
        "This time, we will batch the inputs instead of updating the weights once per epoch.\n",
        "\n",
        "Before starting, to make your experiments reproducible, make sure to [force the random seed](https://pytorch.org/docs/stable/notes/randomness.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f7e402d1-f6f5-4ee5-8cde-813c2a2b4ee1",
      "metadata": {
        "id": "f7e402d1-f6f5-4ee5-8cde-813c2a2b4ee1",
        "outputId": "d199a490-6502-4aae-cf1e-c485d58129da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.4)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Collecting huggingface-hub<1.0.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 KB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n",
            "Collecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.10.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.11.0 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.13.4 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.8.2\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "from copy import deepcopy\n",
        "from functools import partial\n",
        "from typing import Callable, Dict, Generator, List, Tuple\n",
        "\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchtext import vocab\n",
        "from torchtext.vocab import GloVe\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fce0cb2c-05a4-4c70-b2dc-84bcdfadbb6d",
      "metadata": {
        "id": "fce0cb2c-05a4-4c70-b2dc-84bcdfadbb6d"
      },
      "source": [
        "## Using GloVe (1 point)\n",
        "\n",
        "Let's get familier with GloVe embeddings. We download a small version of GloVe trained of 6 billion words, and use vectors of size 300.\n",
        "\n",
        "The [torchtext documentation](https://pytorch.org/text/stable/vocab.html#glove) being quite poor, you can find details on the different pre-trained vectors on the [Stanford page](https://nlp.stanford.edu/projects/glove/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ab2391a1-f23b-4f98-bac4-651081d6c258",
      "metadata": {
        "id": "ab2391a1-f23b-4f98-bac4-651081d6c258",
        "outputId": "b55f4fe7-b34c-45c0-f276-8a20e599c7b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:43, 5.26MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [01:02<00:00, 6399.70it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "glove = GloVe(name=\"6B\", dim=300)\n",
        "len(glove.stoi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f2630885-032e-44ee-9fb7-b7b571216f2a",
      "metadata": {
        "id": "f2630885-032e-44ee-9fb7-b7b571216f2a",
        "outputId": "b232aa4e-db3a-4028-ea20-9e103a66c340",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5450, 'cat', torch.Size([300]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "glove.stoi[\"cat\"], glove.itos[5450], glove.vectors[glove.stoi[\"cat\"]].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b5ef6f37-1d1e-4b3a-82d4-ad09e975aa61",
      "metadata": {
        "id": "b5ef6f37-1d1e-4b3a-82d4-ad09e975aa61",
        "outputId": "05a3ee85-4557-4119-ef7a-715d4243ba32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the', ',', '.', 'of', 'to']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "glove.itos[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8def2d83-d256-4432-ab2a-ab9cf782b062",
      "metadata": {
        "id": "8def2d83-d256-4432-ab2a-ab9cf782b062"
      },
      "source": [
        "Notice that punctuations are part of GloVe's vocabulary.\n",
        "\n",
        "To compare two words, we can look at their cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fca0941f-9e61-4cf0-8bb8-a9132d50f0b7",
      "metadata": {
        "id": "fca0941f-9e61-4cf0-8bb8-a9132d50f0b7",
        "outputId": "95bc66a0-e0ff-420d-afec-1a0512ee5f62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat x cats = 0.6815836429595947\n",
            "cat x dog = 0.6816746592521667\n",
            "cat x fridge = 0.09630905091762543\n"
          ]
        }
      ],
      "source": [
        "words = [\"cat\", \"cats\", \"dog\", \"fridge\"]\n",
        "for word in words[1:]:\n",
        "    similarity = torch.cosine_similarity(\n",
        "        glove.vectors[glove.stoi[words[0]]].reshape(1, -1),\n",
        "        glove.vectors[glove.stoi[word]].reshape(1, -1),\n",
        "    ).item()  # .item() is used to turn a tensor of a single value to a float\n",
        "    print(f\"{words[0]} x {word} = {similarity}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f960f72-be8e-4f35-8b71-7d188ad61a90",
      "metadata": {
        "id": "0f960f72-be8e-4f35-8b71-7d188ad61a90"
      },
      "source": [
        "**\\[1 point\\] Find the closest word to \"cat\" in the whole vocabulary.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max = 0\n",
        "max_element = \"\"\n",
        "for word in glove.itos:\n",
        "  if word == \"cat\":\n",
        "    continue\n",
        "  similarity = torch.cosine_similarity(glove.vectors[glove.stoi[\"cat\"]].reshape(1, -1), glove.vectors[glove.stoi[word]].reshape(1, -1),).item()\n",
        "  if max < similarity:\n",
        "    max = similarity\n",
        "    max_element = word\n",
        "word, max\n"
      ],
      "metadata": {
        "id": "fLG81TTVREXv",
        "outputId": "71e9c800-568c-42a3-8c08-3b2141968079",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "fLG81TTVREXv",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('sandberger', 0.6816746592521667)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2caa4d00-769b-4304-8010-3721bc926e68",
      "metadata": {
        "id": "2caa4d00-769b-4304-8010-3721bc926e68"
      },
      "source": [
        "## Dataset and split\n",
        "\n",
        "As we keep the test set for final evaluation, we need to split the training set into a training and validation set. We make sure the split is **stratified** by class (same proportion of class in each split)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee47710f-8f8f-4e1b-a3e0-390de0a64d4e",
      "metadata": {
        "id": "ee47710f-8f8f-4e1b-a3e0-390de0a64d4e"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"imdb\")\n",
        "train_dataset = dataset[\"train\"].train_test_split(\n",
        "    stratify_by_column=\"label\", test_size=0.2, seed=42\n",
        ")\n",
        "test_df = dataset[\"test\"]\n",
        "train_df = train_dataset[\"train\"]\n",
        "valid_df = train_dataset[\"test\"]\n",
        "train_df.shape, valid_df.shape, test_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8f188d7-9e67-4b03-bab1-00508db56399",
      "metadata": {
        "id": "e8f188d7-9e67-4b03-bab1-00508db56399"
      },
      "source": [
        "## Prepare the inputs\n",
        "\n",
        "### Text processing pipeline (2 points)\n",
        "\n",
        "For a given entry, we want to\n",
        "1. Tokenize the text.\n",
        "2. Get the vectors for each token.\n",
        "3. Average them.\n",
        "\n",
        "For tokenization, let's use the \"basic_english\" tokenizer from torchtext."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9868b71-5bc5-429c-9cb1-4ddb9c331f97",
      "metadata": {
        "id": "a9868b71-5bc5-429c-9cb1-4ddb9c331f97"
      },
      "outputs": [],
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\", language=\"en\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63219cef-b375-40de-b3ad-1401b6079c5a",
      "metadata": {
        "id": "63219cef-b375-40de-b3ad-1401b6079c5a"
      },
      "source": [
        "**\\[2 points\\] Fill the `preprocess_text` function so it returns the mean of the GloVe vectors of all the tokens within a review.**\n",
        "\n",
        "The two following functions can help.\n",
        "* [torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html)\n",
        "* [torch.mean](https://pytorch.org/docs/stable/generated/torch.mean.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ba4642e-fd7c-4dd1-9799-74360787976d",
      "metadata": {
        "id": "3ba4642e-fd7c-4dd1-9799-74360787976d"
      },
      "outputs": [],
      "source": [
        "def vectorize_text(\n",
        "    text: str, vocabulary: vocab.Vocab, tokenizer: Callable[[str], List[str]]\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Turn a string into the average of the vectors of its tokens.\n",
        "    Args:\n",
        "        text: the input text.\n",
        "        vocabulary: a pre-trained Vocab object.\n",
        "        tokenizer: a tokenizer taking a text as input and returning a list of tokens.\n",
        "    Returns:\n",
        "        The average tensor over the tokens of the whole text.\n",
        "    \"\"\"\n",
        "    # Your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b6fa0c3-cb31-49d4-9869-fef8f1bb6a2d",
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "9b6fa0c3-cb31-49d4-9869-fef8f1bb6a2d"
      },
      "outputs": [],
      "source": [
        "text_pipeline = partial(vectorize_text, vocabulary=glove, tokenizer=tokenizer)\n",
        "assert text_pipeline(\"some text.\").shape == torch.Size([300])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3caf280-7436-44b9-a223-c00012a590d2",
      "metadata": {
        "id": "c3caf280-7436-44b9-a223-c00012a590d2"
      },
      "source": [
        "Now we turn our 3 sets into vectors and labels.\n",
        "\n",
        "Our data are quite small, so we can keep everything in RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a6f4e6e-6bc6-4bdf-831d-02b6f14a56b0",
      "metadata": {
        "id": "6a6f4e6e-6bc6-4bdf-831d-02b6f14a56b0"
      },
      "outputs": [],
      "source": [
        "X_train = [text_pipeline(text) for text in tqdm(train_df[\"text\"])]\n",
        "y_train = train_df[\"label\"]\n",
        "X_valid = [text_pipeline(text) for text in tqdm(valid_df[\"text\"])]\n",
        "y_valid = valid_df[\"label\"]\n",
        "X_test = [text_pipeline(text) for text in tqdm(test_df[\"text\"])]\n",
        "y_test = test_df[\"label\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "928a5323-3f1e-4d1e-a5fa-2d752c5ae929",
      "metadata": {
        "id": "928a5323-3f1e-4d1e-a5fa-2d752c5ae929"
      },
      "source": [
        "### Batch processing (1 point)\n",
        "\n",
        "Instead of doing one update per epoch, we feed the model batches of texts between each update. To do so, we use a simple data generator.\n",
        "\n",
        "**\\[1 point\\] Fill the generator function.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "014d3de9-94bb-4be8-9419-8d2aa59ba3c2",
      "metadata": {
        "id": "014d3de9-94bb-4be8-9419-8d2aa59ba3c2"
      },
      "outputs": [],
      "source": [
        "def data_generator(\n",
        "    X: List[torch.tensor], y: List[int], batch_size: int = 32\n",
        ") -> Generator[Tuple[torch.Tensor, torch.Tensor], None, None]:\n",
        "    \"\"\"\n",
        "    Yield batches from given input data and labels.\n",
        "    Args:\n",
        "        X: a list of tensor (input features).\n",
        "        y: the corresponding labels.\n",
        "        batch_size: the size of every batch [32].\n",
        "    Returns:\n",
        "        A tuple of tensors (features, labels).\n",
        "    \"\"\"\n",
        "    X, y = shuffle(X, y)\n",
        "    # Your code\n",
        "    # yield the the returning values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04ab809e-7363-4996-a064-291cd97b276e",
      "metadata": {
        "id": "04ab809e-7363-4996-a064-291cd97b276e"
      },
      "outputs": [],
      "source": [
        "train_gen = lambda: data_generator(X_train, y_train, batch_size=32)\n",
        "for X, y in train_gen():\n",
        "    assert X.shape == torch.Size([32, 300])\n",
        "    assert y.shape == torch.Size([32])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec152a5b-e80e-4d34-82dc-75012707a160",
      "metadata": {
        "id": "ec152a5b-e80e-4d34-82dc-75012707a160"
      },
      "source": [
        "## The classifier (1 point)\n",
        "\n",
        "We create a very simple classifier corresponding a logistic regression.\n",
        "\n",
        "**\\[1 point\\] Fill the classifier's code. The forward function needs to return a logit and not the output of a sigmoid.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09607378-c4a4-4e6b-8bf0-b1c40ef941c4",
      "metadata": {
        "id": "09607378-c4a4-4e6b-8bf0-b1c40ef941c4"
      },
      "outputs": [],
      "source": [
        "class SimpleClassifer(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple linear classifier.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_size: int, nb_classes: int) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_size: the dimension of the input embeddings.\n",
        "        nb_classes: the output dimension.\n",
        "        \"\"\"\n",
        "        # your code\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: an input tensor\n",
        "        Returns:\n",
        "            Logits.\n",
        "        \"\"\"\n",
        "        # your code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5482c601-8ad2-4244-aa49-380df76091c1",
      "metadata": {
        "id": "5482c601-8ad2-4244-aa49-380df76091c1"
      },
      "source": [
        "## Training (3 points)\n",
        "\n",
        "We put everything above together and train the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e6a6da3-d538-4ebc-9bc9-cdec010de29f",
      "metadata": {
        "id": "0e6a6da3-d538-4ebc-9bc9-cdec010de29f"
      },
      "outputs": [],
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47d80e77-75dc-4aa6-b0cf-0f4619bb8b52",
      "metadata": {
        "id": "47d80e77-75dc-4aa6-b0cf-0f4619bb8b52"
      },
      "outputs": [],
      "source": [
        "train_gen = lambda: data_generator(X_train, y_train)\n",
        "valid_gen = lambda: data_generator(X_valid, y_valid)\n",
        "test_gen = lambda: data_generator(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18ddcbc0-a8a0-433f-86d6-63d51f1877c6",
      "metadata": {
        "id": "18ddcbc0-a8a0-433f-86d6-63d51f1877c6"
      },
      "source": [
        "**\\[3 points\\] Fill the following cells. Make sure you save the best model evaluated on the validation set.**\n",
        "* The `deepcopy` function might help."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4ea0bd2-d156-48e4-ab10-82dae2a880f1",
      "metadata": {
        "id": "c4ea0bd2-d156-48e4-ab10-82dae2a880f1"
      },
      "outputs": [],
      "source": [
        "model = SimpleClassifer(300, 1).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "# You can use another optimizer if you want.\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "262b4d32-9d63-4764-92e1-fa7ee27891f4",
      "metadata": {
        "id": "262b4d32-9d63-4764-92e1-fa7ee27891f4"
      },
      "outputs": [],
      "source": [
        "nb_epochs = 50\n",
        "train_losses, valid_losses = [], []\n",
        "\n",
        "best_model = model\n",
        "best_validation_loss = np.Inf\n",
        "\n",
        "for epoch in tqdm(range(nb_epochs)):\n",
        "\n",
        "    # training\n",
        "    model.train()\n",
        "    # training loop\n",
        "    \n",
        "    # validation\n",
        "    model.eval()\n",
        "    # validation loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7907de12-1fc2-4597-980b-87d79baeb807",
      "metadata": {
        "id": "7907de12-1fc2-4597-980b-87d79baeb807"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_losses, label=\"train loss\")\n",
        "plt.plot(valid_losses, label=\"valid loss\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3020533b-0522-42ff-bcdc-6d1cd991d332",
      "metadata": {
        "id": "3020533b-0522-42ff-bcdc-6d1cd991d332"
      },
      "source": [
        "## Evaluation (3 point)\n",
        "\n",
        "**\\[1 point\\] Compute the accuracy for the 3 splits (training, validation, test).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73674e1e-0be6-49b0-af23-a0a9b518d170",
      "metadata": {
        "id": "73674e1e-0be6-49b0-af23-a0a9b518d170"
      },
      "outputs": [],
      "source": [
        "# Your code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac0b7102-fb1d-4a27-a400-ff0e40e1a685",
      "metadata": {
        "id": "ac0b7102-fb1d-4a27-a400-ff0e40e1a685"
      },
      "source": [
        "**\\[1 point\\] For two wrongly classified samples, try guessing why the model was wrong.**\n",
        "\n",
        "**\\[1 point\\] Code a `predict` function which take some text as input and returns a prediction class and score (the output of the sigmoid).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "094f7ac3-ad53-43e3-a1f6-90ed3a61259c",
      "metadata": {
        "id": "094f7ac3-ad53-43e3-a1f6-90ed3a61259c"
      },
      "outputs": [],
      "source": [
        "def predict(\n",
        "    text: str,\n",
        "    text_pipeline: Callable[[str], torch.Tensor],\n",
        "    model: nn.Module,\n",
        "    device: str,\n",
        ") -> Tuple[int, float]:\n",
        "    \"\"\"\n",
        "    Return the predicted class and score for a given input.\n",
        "    Args:\n",
        "        text: a given review.\n",
        "        text_pipeline: a function taking a text as input and returning a tensor (model's input).\n",
        "        model: a pre-trained model.\n",
        "        device: the device on which the computation occurs.\n",
        "    Returns:\n",
        "        A tuple (label, score).\n",
        "    \"\"\"\n",
        "    # Your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84e7130f-ce7f-41ed-a7d2-bcc6f65482f5",
      "metadata": {
        "id": "84e7130f-ce7f-41ed-a7d2-bcc6f65482f5"
      },
      "outputs": [],
      "source": [
        "text = \"In my long years as a movie reviewers, I have seen good and bad movies. But nothing as controversially in the middle.\"\n",
        "predict(text, text_pipeline, model, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c44f2dff-da04-466e-bf2c-32f9e84d10a3",
      "metadata": {
        "id": "c44f2dff-da04-466e-bf2c-32f9e84d10a3"
      },
      "source": [
        "## Bonus\n",
        "\n",
        "Modify the classifier. Instead of using a simple logistic regression, create a multilayer perceptron. Something like `input -> linear(embedding_size, 128) -> activation function -> linear(128, nb_classes) -> output`, for a two layer perceptron.\n",
        "\n",
        "For the activation function, you can use [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU) or [another non-linear activation function](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) of your choice.\n",
        "\n",
        "Train your new classifier, look at the loss, and compare its accuracy with the logistic regression. Keep the model with the best validation loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8975c53d-2707-4d3c-ab8e-6962d5256136",
      "metadata": {
        "id": "8975c53d-2707-4d3c-ab8e-6962d5256136"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}